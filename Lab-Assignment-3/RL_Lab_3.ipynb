{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL-3_211022001.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## <u> RL Lab Assignment - 3 ( Value Iteration and Policy Iteration) </u>\n",
        "\n",
        "### Name : Arvind Pandit\n",
        "### Roll No. : 211022001\n",
        "\n",
        "### <u> Value Iteration Algorithm </u>\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1dbKugsiHLb_UuaxqikpkAcunyiWy5Qxd)\n",
        "\n",
        "### <u> Policy Iteration Algorithm </u>\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1LqsCcUE8KUjCWlIqu1q2FPwk1MtqpKiL)\n",
        "\n",
        "\n",
        "Reference: [Value iteration](http://incompleteideas.net/book/ebook/node44.html) [Policy Iteration](http://incompleteideas.net/book/ebook/node43.html)\n"
      ],
      "metadata": {
        "id": "WaUVSSUk79U2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1.\n",
        "\n",
        "Here we have taken a very simple problem of Markov Decision Process.\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1H-JZZ_DjtxIfZres9OOu8SasgC8SYkTC)\n",
        "\n",
        "\n",
        "There are five 2D tiles which represent the state of MDP.\n",
        "\n",
        "State = {0,1,2,3,4,5}\n",
        "\n",
        "The reward of each state is given by [-1, -1, 10, -1, -1] and the goal is to reach terminal state 3rd which have reward of 10.\n",
        "\n",
        "Reward = {-1, -1, 10, -1, -1}\n",
        "\n",
        "The allowed action are left and right\n",
        "\n",
        "Action = {0,1} where 0=left and 1=right\n",
        "\n",
        "Here the agent have to collect the maximum reward and reach to the terminal state 3rd."
      ],
      "metadata": {
        "id": "CtbSVHANVgKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "## Problem definations ###\n",
        "\n",
        "n=5 # no. of states {0-...-4}\n",
        "m=2 # no. of actions 0-left 1-right\n",
        "rewards = np.array([-1, -1, 10, -1, -1]) # Reward in each state\n",
        "\n",
        "# MDP settings\n",
        "\n",
        "epsilon = 0.004 # small value \n",
        "policy_iter=10000\n",
        "value_iter=10000\n",
        "r=0.9 # discount factor gamma"
      ],
      "metadata": {
        "id": "KGMWwfG3zwOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transition Probability Matrix\n",
        "\n",
        "Prob_a1 = np.matrix([[0.9, 0.1, 0, 0, 0], \n",
        "                     [0.9, 0, 0.1, 0, 0],\n",
        "                     [0, 0, 0, 0, 0],\n",
        "                     [0, 0, 0.9, 0, 0.1],\n",
        "                     [0, 0, 0, 0.9, 0.1]])\n",
        "\n",
        "Prob_a2 = np.matrix([[0.1, 0.9, 0, 0, 0], \n",
        "                     [0.1, 0, 0.9, 0, 0],\n",
        "                     [0, 0, 0, 0, 0],\n",
        "                     [0, 0, 0.1, 0, 0.9],\n",
        "                     [0, 0, 0, 0.1, 0.9]])\n",
        "\n",
        "print(\"Probability Transition matrix for action a1-left\\n\")\n",
        "print(Prob_a1)\n",
        "print(\"\\n\")\n",
        "print(\"Probability Transition matrix for action a2-right\\n\")\n",
        "print(Prob_a2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcWwgnHWipLB",
        "outputId": "30c3b54a-b8d1-4f90-b957-c267708d428b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability Transition matrix for action a1-left\n",
            "\n",
            "[[0.9 0.1 0.  0.  0. ]\n",
            " [0.9 0.  0.1 0.  0. ]\n",
            " [0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.9 0.  0.1]\n",
            " [0.  0.  0.  0.9 0.1]]\n",
            "\n",
            "\n",
            "Probability Transition matrix for action a2-right\n",
            "\n",
            "[[0.1 0.9 0.  0.  0. ]\n",
            " [0.1 0.  0.9 0.  0. ]\n",
            " [0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.1 0.  0.9]\n",
            " [0.  0.  0.  0.1 0.9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Value Iteration"
      ],
      "metadata": {
        "id": "9D6HTncH-aSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states = np.arange(n)\n",
        "action = np.arange(m) \n",
        "P = np.array([Prob_a1, Prob_a2]) # to access Prob[action][intial][next]\n",
        "V_prev=np.zeros(n) # Initialize value fucntion as 0\n",
        "mu = [None, None, None, None, None] # Initialize policy\n",
        "for i in range(value_iter):\n",
        "  max_diff=0 # Initialize max_diff\n",
        "  diff_of_value_function=np.zeros(n) # difference between current value function and previous value fucntion\n",
        "  V_curr = np.zeros(n) # intialize current value fucntion as zero\n",
        "\n",
        "  for s in states:\n",
        "    total_exp_reward = np.zeros(m) # intilize reward as zero\n",
        "\n",
        "    for a in action: \n",
        "      total_exp_reward[a] = rewards[s] + r*np.sum([P[a][s][next_state]*V_prev[next_state] for next_state in states]) # Total expected reward collected by taking action a in state s\n",
        "      \n",
        "    V_curr[s]= np.max(total_exp_reward) # according to the bellman equation\n",
        "    mu[s]= np.argmax(total_exp_reward) # get the action which have given max reward\n",
        "    diff_of_value_function[s]=np.abs(V_prev[s]-V_curr[s]) # diff between each element of V\n",
        "\n",
        "  V_prev=V_curr\n",
        "  max_diff = np.max(diff_of_value_function) # bcoz of l-infinity norm\n",
        "\n",
        "  if max_diff < epsilon: # Stop if diff is less than epsilon\n",
        "    print(\"No. of iterations in Value Iterions = \"+str(i + 1))\n",
        "    break\n",
        "\n",
        "print(\"For Value iteration\")\n",
        "print(\"Optimal Value function V* = \"+str(V_curr))\n",
        "print(\"Optimal policy mu* = \"+ str(mu))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8NjmGSC2GLq",
        "outputId": "e4246418-374f-4b1b-ea3d-43f39deecd27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of iterations in Value Iterions = 10\n",
            "For Value iteration\n",
            "Optimal Value function V* = [ 5.67496513  7.61064654 10.          7.61064654  5.67496513]\n",
            "Optimal policy mu* = [1, 1, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Policy Iterations"
      ],
      "metadata": {
        "id": "-vs4IiU7Mn8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V_prev=np.zeros(n) # Initialize arbitary value fucntion as [0 0 0 0 0]\n",
        "mu=np.zeros(n) # Intilize an arbitatry policy [0 0 0 0 0]\n",
        "\n",
        "for i in range(policy_iter):\n",
        "  optimal_policy = True\n",
        "\n",
        "  # Policy Evaluation\n",
        "  \n",
        "  for j in range(value_iter):\n",
        "    max_diff=0 # Initialize max_diff\n",
        "    diff_of_value_function=np.zeros(n) # difference between current value function and previous value fucntion    \n",
        "    V_curr = np.zeros(n) # intialize current value fucntion as zero\n",
        "\n",
        "    for s in states:\n",
        "      V_curr[s]  = rewards[s] + r*np.sum([P[int(mu[s])][s][next_state]*V_prev[next_state] for next_state in states]) # Total expected reward collected by taking action a in state s according to policy mu\n",
        "      diff_of_value_function[s]=np.abs(V_prev[s]-V_curr[s]) # diff between each element of V\n",
        "\n",
        "    V_prev=V_curr\n",
        "    max_diff = np.max(diff_of_value_function) # bcoz of l-infinity norm\n",
        "\n",
        "    if max_diff < epsilon:\n",
        "      print(\"No. of iterations for Vmu\"+str(i)+\" = \"+str(j + 1))\n",
        "      break\n",
        "\n",
        "  # Policy Improvement\n",
        "\n",
        "  for s in states:\n",
        "    maximum_value = V_prev[s]\n",
        "    for a in action:\n",
        "      value  = rewards[s] + r*np.sum([P[a][s][next_state]*V_prev[next_state] for next_state in states]) # Q(u,a) Total expected reward collected by taking action a in state s and then according to policy mu\n",
        "      if value > maximum_value and mu[s] != a and s!=3 :\n",
        "        mu[s]=a\n",
        "        maximum_value = value\n",
        "        optimal_policy = False\n",
        "  \n",
        "  if optimal_policy :\n",
        "    print(\"No. of iterations in Policy Iterions = \"+str(i + 1))\n",
        "    break\n",
        "\n",
        "print(\"For Policy iteration\")\n",
        "print(\"Optimal Value function V* = \"+str(V_curr))\n",
        "print(\"Optimal policy mu* = \"+ str(mu))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb8a9e7-98ea-43ac-d801-ef52f7f51e9d",
        "id": "98FCX0KxRmVf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of iterations for Vmu0 = 49\n",
            "No. of iterations for Vmu1 = 9\n",
            "No. of iterations in Policy Iterions = 2\n",
            "For Policy iteration\n",
            "Optimal Value function V* = [ 5.67450141  7.61052229 10.          7.61079919  5.67554653]\n",
            "Optimal policy mu* = [1. 1. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Optimal Policy is mu=[1 1 0 0 0]  s.t  [right right left left left]\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1k89bXK2ptG4Gd5FRXLq4LxIsJ4xSuqtx)"
      ],
      "metadata": {
        "id": "WruHKem0U0k1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Verification by choosing Random Policy"
      ],
      "metadata": {
        "id": "JgjfU6HO-mE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "############################### Verification by choosing Random Policy ###############################################\n",
        "print(\"Average collected reward with optimal policy mu \"+ str(mu) + \" is \" + str(np.average(V_curr)))\n",
        "for k in range(3):\n",
        "  print(\"\\n\")\n",
        "  rand_mu = np.random.choice(m, n)\n",
        "  for j in range(value_iter):\n",
        "      max_diff=0 # Initialize max_diff\n",
        "      diff_of_value_function=np.zeros(n) # difference between current value function and previous value fucntion    \n",
        "      V_curr = np.zeros(n) # intialize current value fucntion as zero\n",
        "\n",
        "      for s in states:\n",
        "        V_curr[s]  = rewards[s] + r*np.sum([P[int(rand_mu[s])][s][next_state]*V_prev[next_state] for next_state in states]) # Total expected reward collected by taking action a in state s according to policy mu\n",
        "        diff_of_value_function[s]=np.abs(V_prev[s]-V_curr[s]) # diff between each element of V\n",
        "\n",
        "      V_prev=V_curr\n",
        "      max_diff = np.max(diff_of_value_function) # bcoz of l-infinity norm\n",
        "\n",
        "      if max_diff < epsilon:\n",
        "        break\n",
        "  print(\"Average collected reward with random policy mu_\" + str(k) + \" \"+ str(rand_mu) + \" is \" + str(np.average(V_curr)))\n",
        "\n",
        "#################################### End ###################################################################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RV97l6Vu7AA",
        "outputId": "219f363b-ab19-4c0a-d94b-100a08d57676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average collected reward with optimal policy mu [1. 1. 0. 0. 0.] is 7.314273885011818\n",
            "\n",
            "\n",
            "Average collected reward with random policy mu_0 [1 1 1 1 0] is 3.104499557903851\n",
            "\n",
            "\n",
            "Average collected reward with random policy mu_1 [0 1 0 1 1] is -0.13949453794473196\n",
            "\n",
            "\n",
            "Average collected reward with random policy mu_2 [1 0 0 0 1] is 1.432984378709557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2.\n",
        "We have taken a grid problem of Markov Decision Process.\n",
        "\n",
        "Here the robot starts from the state 0 and there are walls in state (2,3,8,9) and the final destination of the robot is state 12.\n",
        "\n",
        "The movement of the robot have the randomness such that it will go to the commanded direction with probability 0.7 and other 3 direction with each having probability 0.1.\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1bIpVHiiPsghTGeZ4f_EGd1CLux6HUlIv)\n",
        "\n",
        "There are 16 grids which represent the state of MDP.\n",
        "\n",
        "State = {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15}\n",
        "\n",
        "The reward of each state is given by {-1,-1,0,0,-1,-1,-1,-1,0,0,-1,-1,10,-1,-1,-1} and the goal is to reach terminal state 12th which have reward of 10.\n",
        "\n",
        "Each step will cost 1.\n",
        "\n",
        "Reward = {-1,-1,0,0,-1,-1,-1,-1,0,0,-1,-1,10,-1,-1,-1}\n",
        "\n",
        "The allowed action are up,down,left and right\n",
        "\n",
        "Action = {0,1,2,3} where 0=up,1=down,2=left,3=right\n",
        "\n",
        "Here the agent have to collect the maximum reward and reach to the terminal state 12 avoiding the collision with walls.\n"
      ],
      "metadata": {
        "id": "KzXV4tiNiy1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "## Problem definations ###\n",
        "\n",
        "n=16 # no. of states {0-...-15}\n",
        "m=4 # no. of actions 0-up,1-down,2-left,3-right\n",
        "rewards = np.array([-1,-1,0,0,-1,-1,-1,-1,0,0,-1,-1,10,-1,-1,-1])\n",
        "\n",
        "## Define wall_states and terminals state\n",
        "\n",
        "wall = np.array([2,3,8,9])\n",
        "terminal=12\n",
        "\n",
        "# MDP settings\n",
        "\n",
        "epsilon = 0.004 # small value \n",
        "policy_iter=10000\n",
        "value_iter=10000\n",
        "r=0.9 # discount factor gamma"
      ],
      "metadata": {
        "id": "z4xMfPQ9xpBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PowerOfTwo(n):\n",
        "    if (n == 0):\n",
        "        return False,0\n",
        "    counter=0\n",
        "    while (n != 1):\n",
        "      if (n % 2 != 0):\n",
        "        return False,0\n",
        "      n = n // 2\n",
        "      counter+=1       \n",
        "    return True,counter"
      ],
      "metadata": {
        "id": "9OgfNG29nRHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states = np.arange(n)\n",
        "action = np.arange(m) \n",
        "P_u = np.zeros((n,n))\n",
        "P_d = np.zeros((n,n))\n",
        "P_l = np.zeros((n,n))\n",
        "P_r = np.zeros((n,n))\n",
        "\n",
        "c=0 # column variable\n",
        "column = PowerOfTwo(n)\n",
        "if(column[0]):\n",
        "    c=column[1]\n",
        "else:\n",
        "    print('State is not a power of 2 please give a square grid problem')\n",
        "\n",
        "P=np.array([P_u,P_d,P_l,P_r])\n",
        "\n",
        "u_l = np.array([z*c+(c-1) for z in range(c)]) # [3,7,11,15]\n",
        "d_l = np.array([z*c for z in range(c)]) #  [0,4,8,12]\n",
        "l_l = np.array([z for z in range(c)]) # [0,1,2,3]\n",
        "r_l = np.array([n-(1+z) for z in range(c)]) # [12,13,14,15]\n",
        "\n",
        "\n",
        "a=0\n",
        "## For action up a=0\n",
        "for s in states:\n",
        "  if s==terminal or s in wall :\n",
        "    P[a][s][s]=1\n",
        "    continue\n",
        "  if s in u_l or s+1 in wall:\n",
        "      P[a][s][s]+=0.7\n",
        "  else:\n",
        "      P[a][s][s+1]=0.7\n",
        "  \n",
        "  if s in d_l:\n",
        "      P[a][s][s]+=0.1\n",
        "  else:\n",
        "      P[a][s][s-1]=0.1\n",
        "  \n",
        "  if s in l_l:\n",
        "      P[a][s][s]+=0.1\n",
        "  else:\n",
        "      P[a][s][s-c]=0.1\n",
        "  \n",
        "  if s in r_l:\n",
        "      P[a][s][s]+=0.1\n",
        "  else:\n",
        "      P[a][s][s+c]=0.1\n",
        "    \n",
        "a=1\n",
        "## For action down a=0\n",
        "for s in states:\n",
        "  if s==terminal:\n",
        "    P[a][s][s]=1\n",
        "    continue\n",
        "  if s in u_l:\n",
        "      P[a][s][s]+=0.1\n",
        "  else:\n",
        "      P[a][s][s+1]=0.1\n",
        "  \n",
        "  if s in d_l or s-1 in wall:\n",
        "      P[a][s][s]+=0.7\n",
        "  else:\n",
        "      P[a][s][s-1]=0.7\n",
        "  \n",
        "  if s in l_l:\n",
        "      P[a][s][s]+=0.1\n",
        "  else:\n",
        "      P[a][s][s-c]=0.1\n",
        "  \n",
        "  if s in r_l:\n",
        "      P[a][s][s]+=0.1\n",
        "  else:\n",
        "      P[a][s][s+c]=0.1\n",
        "\n",
        "a=2\n",
        "## For action left a=0\n",
        "for s in states:\n",
        "  if s==terminal or s in wall :\n",
        "    P[a][s][s]=1\n",
        "    continue\n",
        "  if s in u_l:\n",
        "      P[a][s][s]+=0.1\n",
        "  else:\n",
        "      P[a][s][s+1]=0.1\n",
        "  \n",
        "  if s in d_l:\n",
        "      P[a][s][s]+=0.1\n",
        "  else:\n",
        "      P[a][s][s-1]=0.1\n",
        "  \n",
        "  if s in l_l or s-c in wall:\n",
        "      P[a][s][s]+=0.7\n",
        "  else:\n",
        "      P[a][s][s-c]=0.7\n",
        "  \n",
        "  if s in r_l:\n",
        "      P[a][s][s]+=0.1\n",
        "  else:\n",
        "      P[a][s][s+c]=0.1\n",
        "\n",
        "a=3\n",
        "## For action right a=0\n",
        "for s in states:\n",
        "  if s==terminal or s in wall :\n",
        "    P[a][s][s]=1\n",
        "    continue\n",
        "  if s in u_l:\n",
        "      P[a][s][s]+=0.1\n",
        "  else:\n",
        "      P[a][s][s+1]=0.1\n",
        "  \n",
        "  if s in d_l:\n",
        "      P[a][s][s]+=0.1\n",
        "  else:\n",
        "      P[a][s][s-1]=0.1\n",
        "  \n",
        "  if s in l_l:\n",
        "      P[a][s][s]+=0.1\n",
        "  else:\n",
        "      P[a][s][s-c]=0.1\n",
        "  \n",
        "  if s in r_l or s+c in wall:\n",
        "      P[a][s][s]+=0.7\n",
        "  else:\n",
        "      P[a][s][s+c]=0.7\n",
        "\n",
        "print(\"Probability Transition matrix for action a1-up\\n\")\n",
        "print(P[0])\n",
        "print(\"\\n\")\n",
        "print(\"Probability Transition matrix for action a2-down\\n\")\n",
        "print(P[1])\n",
        "print(\"\\n\")\n",
        "print(\"Probability Transition matrix for action a3-left\\n\")\n",
        "print(P[2])\n",
        "print(\"\\n\")\n",
        "print(\"Probability Transition matrix for action a4-right\\n\")\n",
        "print(P[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeUU5KZglLiW",
        "outputId": "9f4e49b3-7adb-4d47-8c46-32cda914cb94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability Transition matrix for action a1-up\n",
            "\n",
            "[[0.2 0.7 0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.1 0.8 0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.1 0.  0.  0.  0.1 0.7 0.  0.  0.1 0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.1 0.  0.  0.1 0.  0.7 0.  0.  0.1 0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.1 0.  0.  0.1 0.  0.7 0.  0.  0.1 0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.1 0.  0.  0.1 0.7 0.  0.  0.  0.1 0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.1 0.  0.7 0.  0.  0.1 0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.1 0.7 0.  0.  0.  0.1]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.1 0.1 0.7 0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.1 0.1 0.7]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.1 0.8]]\n",
            "\n",
            "\n",
            "Probability Transition matrix for action a2-down\n",
            "\n",
            "[[0.8 0.1 0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.7 0.1 0.1 0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.7 0.1 0.1 0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.9 0.  0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.1 0.  0.  0.  0.7 0.1 0.  0.  0.1 0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.1 0.  0.  0.7 0.  0.1 0.  0.  0.1 0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.1 0.  0.  0.7 0.  0.1 0.  0.  0.1 0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.1 0.  0.  0.7 0.1 0.  0.  0.  0.1 0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.1 0.  0.  0.  0.7 0.1 0.  0.  0.1 0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.7 0.1 0.  0.  0.1 0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.  0.7 0.1 0.  0.  0.1 0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.7 0.1 0.  0.  0.  0.1]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.7 0.1 0.1 0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.7 0.1 0.1]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.7 0.2]]\n",
            "\n",
            "\n",
            "Probability Transition matrix for action a3-left\n",
            "\n",
            "[[0.8 0.1 0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.1 0.7 0.1 0.  0.  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.7 0.  0.  0.  0.1 0.1 0.  0.  0.1 0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.7 0.  0.  0.1 0.  0.1 0.  0.  0.1 0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.1 0.7 0.1 0.  0.  0.1 0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.1 0.8 0.  0.  0.  0.1 0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.7 0.  0.  0.1 0.  0.1 0.  0.  0.1 0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.7 0.  0.  0.1 0.1 0.  0.  0.  0.1]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.8 0.1 0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.7 0.  0.  0.1 0.1 0.1]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.7 0.  0.  0.1 0.2]]\n",
            "\n",
            "\n",
            "Probability Transition matrix for action a4-right\n",
            "\n",
            "[[0.2 0.1 0.  0.  0.7 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.1 0.1 0.1 0.  0.  0.7 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.1 0.  0.  0.  0.8 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.1 0.  0.  0.1 0.7 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.1 0.  0.  0.1 0.  0.1 0.  0.  0.7 0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.1 0.  0.  0.1 0.1 0.  0.  0.  0.7 0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.1 0.  0.1 0.  0.  0.7 0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.1 0.1 0.  0.  0.  0.7]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.1 0.7 0.1 0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.1 0.7 0.1]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.  0.  0.1 0.8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Value Iteration"
      ],
      "metadata": {
        "id": "ZRu6nv0lmIze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V_prev=np.zeros(n) # Initialize value fucntion as 0\n",
        "mu = [None for i in range(n)] # Initialize policy\n",
        "for i in range(value_iter):\n",
        "  max_diff=0 # Initialize max_diff\n",
        "  diff_of_value_function=np.zeros(n) # difference between current value function and previous value fucntion\n",
        "  V_curr = np.zeros(n) # intialize current value fucntion as zero\n",
        "\n",
        "  for s in states:\n",
        "    total_exp_reward = np.zeros(m) # intilize reward as zero\n",
        "\n",
        "    for a in action: \n",
        "      total_exp_reward[a] = rewards[s] + r*np.sum([P[a][s][next_state]*V_prev[next_state] for next_state in states]) # Total expected reward collected by taking action a in state s\n",
        "      \n",
        "    V_curr[s]= np.max(total_exp_reward) # according to the bellman equation\n",
        "    mu[s]= np.argmax(total_exp_reward) # get the action which have given max reward\n",
        "    diff_of_value_function[s]=np.abs(V_prev[s]-V_curr[s]) # diff between each element of V\n",
        "\n",
        "  V_prev=V_curr\n",
        "  max_diff = np.max(diff_of_value_function) # bcoz of l-infinity norm\n",
        "\n",
        "  if max_diff < epsilon: # Stop if diff is less than epsilon\n",
        "    print(\"No. of iterations in Value Iterions = \"+str(i + 1))\n",
        "    break\n",
        "\n",
        "# print(V_curr)\n",
        "# print(mu)\n",
        "\n",
        "policy = []\n",
        "for p in mu:\n",
        "  if p==0:\n",
        "    policy.append(\"up\")\n",
        "  elif p==1:\n",
        "    policy.append(\"down\")\n",
        "  elif p==2:\n",
        "    policy.append(\"left\")\n",
        "  else:\n",
        "    policy.append(\"right\")\n",
        "\n",
        "policy[terminal]=\"nan\"\n",
        "for w in wall:\n",
        "  policy[int(w)]=\"nan\"\n",
        "\n",
        "\n",
        "print(\"For Value iteration\")\n",
        "print(\"Optimal Value function V* = \"+str(V_curr))\n",
        "print(\"Optimal policy mu* = \"+ str(mu))\n",
        "print(\"Optimal policy\"+str(policy))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFZdCrrjjlo8",
        "outputId": "40af0c5e-4df8-4663-e473-2b97f0b5d6dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of iterations in Value Iterions = 76\n",
            "For Value iteration\n",
            "Optimal Value function V* = [21.89782805 24.80772769 22.53287879 16.14270632 26.55055866 31.07872535\n",
            " 38.0726069  34.11590598 40.15483901 38.60073338 49.49279352 43.12606991\n",
            " 99.96670104 78.15737539 63.03837479 51.94169849]\n",
            "Optimal policy mu* = [3, 3, 1, 1, 0, 0, 3, 3, 1, 1, 3, 3, 0, 1, 1, 1]\n",
            "Optimal policy['right', 'right', 'nan', 'nan', 'up', 'up', 'right', 'right', 'nan', 'nan', 'right', 'right', 'nan', 'down', 'down', 'down']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Policy Iteration"
      ],
      "metadata": {
        "id": "ZcX2tj82mOqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mu=np.zeros(n) # Intilize an arbitatry policy\n",
        "\n",
        "for i in range(policy_iter):\n",
        "  optimal_policy = True\n",
        "\n",
        "  # Policy Evaluation\n",
        "  \n",
        "  for j in range(value_iter):\n",
        "    max_diff=0 # Initialize max_diff\n",
        "    diff_of_value_function=np.zeros(n) # difference between current value function and previous value fucntion    \n",
        "    V_curr = np.zeros(n) # intialize current value fucntion as zero\n",
        "\n",
        "    for s in states:\n",
        "      V_curr[s]  = rewards[s] + r*np.sum([P[int(mu[s])][s][next_state]*V_prev[next_state] for next_state in states]) # Total expected reward collected by taking action a in state s according to policy mu\n",
        "      diff_of_value_function[s]=np.abs(V_prev[s]-V_curr[s]) # diff between each element of V\n",
        "\n",
        "    V_prev=V_curr\n",
        "    max_diff = np.max(diff_of_value_function) # bcoz of l-infinity norm\n",
        "\n",
        "    if max_diff < epsilon:\n",
        "      print(\"No. of iterations for Vmu\"+str(i)+\" = \"+str(j + 1))\n",
        "      break\n",
        "\n",
        "  # Policy Improvement\n",
        "\n",
        "  for s in states:\n",
        "    maximum_value = V_prev[s]\n",
        "    for a in action:\n",
        "      value  = rewards[s] + r*np.sum([P[a][s][next_state]*V_prev[next_state] for next_state in states]) # Total expected reward collected by taking action a in state s according to policy mu\n",
        "      if value > maximum_value and mu[s] != a and s!=terminal :\n",
        "        mu[s]=a\n",
        "        maximum_value = value\n",
        "        optimal_policy = False\n",
        "  # print(mu)\n",
        "  if optimal_policy :\n",
        "    print(\"No. of iterations in Policy Iterions = \"+str(i + 1))\n",
        "    break\n",
        "\n",
        "\n",
        "\n",
        "# print(V_curr)\n",
        "# print(mu)\n",
        "\n",
        "policy = []\n",
        "for p in mu:\n",
        "  if p==0:\n",
        "    policy.append(\"up\")\n",
        "  elif p==1:\n",
        "    policy.append(\"down\")\n",
        "  elif p==2:\n",
        "    policy.append(\"left\")\n",
        "  else:\n",
        "    policy.append(\"right\")\n",
        "\n",
        "policy[terminal]=\"nan\"\n",
        "for w in wall:\n",
        "  policy[int(w)]=\"nan\"\n",
        "\n",
        "print(\"For Value iteration\")\n",
        "print(\"Optimal Value function V* = \"+str(V_curr))\n",
        "print(\"Optimal policy mu* = \"+ str(mu))\n",
        "print(\"Optimal policy\"+str(policy))\n"
      ],
      "metadata": {
        "id": "n9W91jSfrScJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b77f1a5b-db5b-4e92-8a80-17972aab5800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of iterations for Vmu0 = 67\n",
            "No. of iterations for Vmu1 = 28\n",
            "No. of iterations for Vmu2 = 45\n",
            "No. of iterations for Vmu3 = 36\n",
            "No. of iterations in Policy Iterions = 4\n",
            "For Value iteration\n",
            "Optimal Value function V* = [ 21.92829068  24.83823287  22.5612402   16.15729915  26.58141653\n",
            "  31.10992607  38.10416901  34.14586757  40.18609368  38.63246463\n",
            "  49.52532705  43.15841139 100.          78.19043061  63.07129906\n",
            "  51.9744841 ]\n",
            "Optimal policy mu* = [3. 3. 1. 1. 0. 0. 3. 3. 1. 1. 3. 3. 0. 1. 1. 1.]\n",
            "Optimal policy['right', 'right', 'nan', 'nan', 'up', 'up', 'right', 'right', 'nan', 'nan', 'right', 'right', 'nan', 'down', 'down', 'down']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=1OBD-npliwSZ1nswxDuBaUbMsFvqw58s2)\n"
      ],
      "metadata": {
        "id": "IBfAaEthlqpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Verification by choosing Random Policy"
      ],
      "metadata": {
        "id": "jH4S0u8Y-8RS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############################### Verification by choosing Random Policy ###############################################\n",
        "print(\"Verification by choosing Random Policy\")\n",
        "print(\"Average collected reward with optimal policy mu \"+ str(mu) + \" is \" + str(np.average(V_curr)))\n",
        "for k in range(5):\n",
        "  print(\"\\n\")\n",
        "  rand_mu = np.random.choice(m,n)\n",
        "  for j in range(value_iter):\n",
        "      max_diff=0 # Initialize max_diff\n",
        "      diff_of_value_function=np.zeros(n) # difference between current value function and previous value fucntion    \n",
        "      V_curr = np.zeros(n) # intialize current value fucntion as zero\n",
        "\n",
        "      for s in states:\n",
        "        V_curr[s]  = rewards[s] + r*np.sum([P[int(rand_mu[s])][s][next_state]*V_prev[next_state] for next_state in states]) # Total expected reward collected by taking action a in state s according to policy mu\n",
        "        diff_of_value_function[s]=np.abs(V_prev[s]-V_curr[s]) # diff between each element of V\n",
        "\n",
        "      V_prev=V_curr\n",
        "      max_diff = np.max(diff_of_value_function) # bcoz of l-infinity norm\n",
        "\n",
        "      if max_diff < epsilon:\n",
        "        break\n",
        "  print(\"Average collected reward with random policy mu \"+ str(rand_mu) + \" is \" + str(np.average(V_curr)))"
      ],
      "metadata": {
        "id": "pbYU3kj3lz4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "668809bd-2963-4cd0-8961-5c01bc3c23cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification by choosing Random Policy\n",
            "Average collected reward with optimal policy mu [3. 3. 1. 1. 0. 0. 3. 3. 1. 1. 3. 3. 0. 1. 1. 1.] is 42.51030953722727\n",
            "\n",
            "\n",
            "Average collected reward with random policy mu [3 0 3 2 0 1 2 2 1 2 3 0 1 1 1 1] is 23.18138200974328\n",
            "\n",
            "\n",
            "Average collected reward with random policy mu [1 3 3 0 0 1 3 1 2 2 0 2 2 1 0 2] is 7.472441067717258\n",
            "\n",
            "\n",
            "Average collected reward with random policy mu [3 0 1 1 2 0 0 0 0 3 0 0 0 1 2 1] is 6.843661265486373\n",
            "\n",
            "\n",
            "Average collected reward with random policy mu [1 2 2 1 3 3 0 2 3 0 3 3 0 2 3 3] is 4.833853267800601\n",
            "\n",
            "\n",
            "Average collected reward with random policy mu [3 3 0 1 0 0 2 0 0 3 1 1 1 2 0 1] is 4.288022366768863\n"
          ]
        }
      ]
    }
  ]
}